\chapter{Convex Functions}
\label{Ch:ConvexFunctions}


\section{Optimization problem: set-up and its solutions}
\label{Ch:ConvexFunctions:Set-up}

For the objective function $f: \R^n \supset\cD\ra \R$, we have various notions of minimizers: 

\begin{itemize}
\item Local minimizer: $x^*\in \cD$ is a local minimizer if there exist a neighborhood $\cN$ containing $x^*$, such that $f(x)\geq f(x^*)$ for all $x\in \cN \cap \cD$;

\item Global minimizer: $x^*\in \cD$ is a global minimizer if $f(x)\geq f(x^*)$ for all $x\in \cD$;

\item Strict Local Minimizer: $x^*\in \cD$ is a strict local minimizer iff $x^*$ is a local minimizer and $f(x) > f(x^*)$ if $x\neq x^*$, $x\in \cN$;

\item Isolated Local Minimizer: $x^*\in \cD$ is an isolated local minimizer iff there exists a neighborhood $\cN$ containing $x^*$ such that $f(x)\geq f(x^*)$ for all $x\in \cN\cap \cD$ and $\cN$ does not contain any other local minimizers.
\end{itemize}

Constrained optimization problem
\begin{equation}\label{Eq:ConstrainedOptimization}
\min\li_{x\in \Om}f(x) \ ,
\end{equation}
where $\Om\subset \cD\subset \R^n$ is a closed set.

$x^*$ is a \textit{local solution}: there exist a neighborhood $\cN$ containing $x^*$ such that $f(x)\geq f(x^*)$ for any $x\in \cN\cap \Om$.

$x^*$ is a \textit{global solution}: $f(x)\geq f(x^*)$ for any $x\in \Om$.

\section{Convexity}
\label{Ch:ConvexFunctions:Convexity}

\begin{definition}[Convex Set]\label{Def:ConvexSet}
A set $\Om$ is called a convex set if for any $x,y\in \Om$ we have $$(1-\al)x+\al y\in \Om$$ for all $\al\in [0,1]$.
\end{definition}

Given convex set $\Om\subset \R^n$, the \textit{projection operator} $P: \R^n\ra \Om$ is given by
\begin{equation}\label{Eq:ProjectionOperator}
P(y)=\arg\min\li_{z\in \Om}\|z-y\|_2^2 \ .
\end{equation}
$P(y)$ is the point in $\Om$ that is closest to $y$ in the sense of Euclidean norm. If $x\in \Om$, then $P(x)=x$.

\begin{definition}[Convex Function]\label{Def:ConvexFunction}
A function $\phi: \R^n\ra \R\cup \{\pm \infty\}$ is convex if for all $x,y\in \R^n$ we have 
\begin{equation}\label{Eq:ConvexFunction}
\phi((1-\al)x+\al y)\leq (1-\al)\phi(x)+\al \phi(y)
\end{equation}
for all $\al \in [0,1]$.
\end{definition}

\begin{definition}[Strongly Convex Function]\label{Def:StronglyConvexFunction}
A convex function $\phi$ is called strongly convex with modulus of convexity $m>0$ if
\begin{equation}\label{Eq:StronglyConvexFunction}
\phi((1-\al)x+\al y)\leq (1-\al)\phi(x)+\al \phi(y)-\dfrac{1}{2}m\al(1-\al)\|x-y\|_2^2
\end{equation}
for all $x, y$ in the domain of $\phi$.
\end{definition}

Let $\Om\subset \R^n$ be convex. We define the indicator function
\begin{equation}\label{Eq:Indicator}
I_\Om(x)=\left\{\begin{array}{ll}0 & \text{ if } x\in \Om \ , \\ +\infty & \text{ otherwise} \ .\end{array}\right.
\end{equation}
Constrained optimization problem $\min\li_{x\in \Om}f(x)$ is the same thing as the unconstrained optimization problem
$\min [f(x)+I_\Om(x)]$. We can set a sequence of functions $F_{\lb \Om} \uparrow I_\Om(x)$ as $\lb \uparrow \infty$, and then we solve the relaxed problem $\min\li_{x\in \R^d}[f(x)+F_{\lb \Om}(x)]$.

\begin{theorem}[Minimizers for convex functions]\label{Thm:MinimizerConvexFunctions}
If the function $f$ is convex and the set $\Om$ is closed and convex, then

(a) Any local solution of \eqref{Eq:ConstrainedOptimization} is also global;

(b) The set of global solutions of \eqref{Eq:ConstrainedOptimization} is a convex set.
\end{theorem}

\begin{proof}
(a) Suppose $x^*_1$ is a local optimizer that is not global. This means there exists some $x^*_2\neq x^*_1$ such that $f(x^*_2)<f(x^*_1)$. Then by convexity for any $\al\in [0,1]$ we have 
$$f((1-\al)x_1^*+\al x_2^*)\leq (1-\al)f(x_1^*)+\al f(x_2^*)< f(x_1^*) \ .$$
If $\al$ is close to $0$ and the above inequality will violate the fact that $x_1^*$ is a local minimizer.

(b) Let $S$ be the set of global minimizers. Then for any $x_1,x_2\in S$ we have $f(x_1)=f(x_2)=\min\li_{x\in \Om} f(x)$. By convexity we have
$$f(\al x_1+ (1-\al)x_2)\leq \al f(x_1)+(1-\al)f(x_2)=\min\li_{x\in \Om}f(x) \ ,$$
which means that $S$ is convex.
\end{proof}

A few more auxiliary notions:
\begin{itemize}
\item ``Effective domain" of $\phi$: $\{x\in \Om: \phi(x)<\infty\}$;

\item ``Epigraph" of $\phi$: $$\text{epi}(\phi)=\{(x,t)\in \Om\times \R: t\geq \phi(x)\} \ .$$

\item $\phi$ is a ``proper convex function" if $\phi(x)<\infty$ for some $x\in \Om$ and $\phi(x)>-\infty$ for all $x\in \Om$;

\item $\phi$ is a ``closed proper convex function" if $\phi$ is a proper convex function and $\{x\in \Om: \phi(x)\leq t\}$ is a closed set for all $t\in \R$.
\end{itemize}

\section{Taylor's theorem and convexity in Taylor's expansion}
\label{Ch:ConvexFunctions:Taylor}

\begin{theorem}[Taylor's theorem]\label{Thm:Taylor}
Given a continuously differentiable function $f: \R^n\ra \R$ and given $x,p\in \R^n$ we have
\begin{equation}\label{Thm:Taylor:Eq:IntegralForm}
f(x+p)=f(x)+\int_0^1 \grad f(x+\gm p)^Tpd\gm \ ,
\end{equation}
\begin{equation}\label{Thm:Taylor:Eq:MeanValueForm}
f(x+p)=f(x)+\grad f(x+\gm p)^Tp \ , \ \text{ for some } \gm\in (0,1) \ .
\end{equation}
If $f$ is twice continuously differentiable, then
\begin{equation}\label{Thm:Taylor:Eq:GradientIntegralForm}
\grad f(x+p)=\grad f(x)+\int_0^1 \grad^2 f(x+\gm p)^Tpd\gm \ ,
\end{equation}
\begin{equation}\label{Thm:Taylor:Eq:GradientMeanValueForm}
f(x+p)=f(x)+\grad f(x)^Tp+\dfrac{1}{2}p^T\grad^2 f(x+\gm p)p \ , \ \text{ for some } \gm\in (0,1) \ .
\end{equation}
\end{theorem}

\begin{proof}
(1) Let $g(\gm)=f(x+\gm p)$, then $g'(\gm)=\grad f(x+\gm p)\cdot p =(\grad f(x+\gm p))^T p$. Then by Newton-Leibniz we have
$$g(1)-g(0)=\int_0^1 g'(\gm)d\gm = \int_0^1 (\grad f(x+\gm p))^T p d\gm \ ,$$
which gives (\ref{Thm:Taylor:Eq:IntegralForm}).

(2) Using mean-value theorem $\play{\int_a^b h(t)dt} =(b-a)h(\xi)$ for some $\xi\in (a,b)$, we get $$\int_0^1 (\grad f(x+\gm p))^T p d\gm = (\grad f(x+\gm_0 p))^T p$$ for some $\gm_0\in (0,1)$, which is (\ref{Thm:Taylor:Eq:MeanValueForm}).

Another way is to use the fact that $$f(y)=f(x)+(\grad f(x+\gm_0(y-x)))^T(y-x) \ .$$
Set $p=y-x$, this gives
$$f(x+p)=f(x)+(\grad f(x+\gm_0 p))^T p \ .$$

(3) We apply (\ref{Thm:Taylor:Eq:GradientIntegralForm}) to each of $\dfrac{\pt f}{\pt x_i}$, and we get
$$\dfrac{\pt f}{\pt x_i}(x+p)=\dfrac{\pt f}{\pt x_i}(x)+\int_0^1 \left[\grad\left(\dfrac{\pt f}{\pt x_i}\right)(x+\gm p)\right]^T p d \gm \ .$$

Introduce the Hessian matrix $\grad^2 f(x)=\left(\dfrac{\pt ^2 f}{\pt x_i \pt x_j}\right)_{1\leq i, j\leq n}$. Then when we put the above equation for all $1\leq i\leq n$, we get (\ref{Thm:Taylor:Eq:GradientIntegralForm}).

(4) Define an auxiliary function of a single variable $\phi(t) = f(x + tp)$ for $t \in [0, 1]$. By applying the univariate Taylor's Theorem with the Lagrange form of the remainder to $\phi(t)$ at $t=0$, we have:
$$
\phi(1) = \phi(0) + \phi'(0)(1-0) + \frac{1}{2} \phi''(\gamma)(1-0)^2
$$
for some $\gamma \in (0, 1)$.

Now, we compute the derivatives of $\phi(t)$ using the multi-variable chain rule:
$$ \phi'(t) = \frac{d}{dt} f(x + tp) = \nabla f(x + tp)^T p $$
At $t=0$, we obtain $\phi'(0) = \nabla f(x)^T p$.

$$ \phi''(t) = \frac{d}{dt} \left( \nabla f(x + tp)^T p \right) = p^T \nabla^2 f(x + tp) p $$
    where $\nabla^2 f$ is the Hessian matrix of $f$.

Substituting these expressions back into the Taylor expansion of $\phi(1)$:
Since $\phi(1) = f(x+p)$ and $\phi(0) = f(x)$, we get (\ref{Thm:Taylor:Eq:GradientMeanValueForm}).
\end{proof}

\begin{definition}[$L$--Lipschitz]\label{Def:LLipschitz}
If $f: \R^n\ra \R$ is such that
\begin{equation}\label{Eq:L-Lipschitz}
|f(x)-f(y)|\leq L\|x-y\| \ ,
\end{equation}
for $L>0$ and any $x,y\in \R^n$, then $f(x)$ is $L$--Lipschitz.
\end{definition}

Optimization literatures often assume that $\grad f$ is $L$--Lipschitz

\begin{equation}\label{Eq:gradf-L-Lipschitz}
\|\grad f(x)-\grad f(y)\|\leq L\|x-y\| \ .
\end{equation}

\begin{theorem}\label{Thm:ConvexFunctionMainTheorem}
(1) If $f$ is continuously differentiable and convex then
\begin{equation}\label{Thm:ConvexFunctionMainTheorem:Eq:FirstOrder}
f(y)\geq f(x)+(\grad f(x))^T (y-x)
\end{equation}
for any $x,y \in \text{dom}(f)$.

(2) If $f$ is differentiable and $m$--strongly convex then
\begin{equation}\label{Thm:ConvexFunctionMainTheorem:Eq:SecondOrderLowerBound}
f(y)\geq f(x)+(\grad f(x))^T(y-x)+\dfrac{m}{2}\|y-x\|^2
\end{equation}
for any $x,y\in \text{dom}(f)$.

(3) If $\grad f$ is uniformly Lipschitz continuous with Lipschitz constant $L>0$ and $f$ is convex then
\begin{equation}\label{Thm:ConvexFunctionMainTheorem:Eq:SecondOrderUpperBound}
f(y)\leq f(x)+(\grad f(x))^T(y-x)+\dfrac{L}{2}\|y-x\|^2
\end{equation}
for any $x,y\in \text{dom}(f)$.
\end{theorem}

\begin{proof}
(1) Let $z_\al=\al x + (1-\al)y$. Then by convexity
$$f(z_\al)\leq \al f(x) + (1-\al) f(y) \ ,$$
which gives
$$f(z_\al)-f(x)\leq (1-\al)(f(y)-f(x)) \ .$$
We can use (\ref{Thm:Taylor:Eq:MeanValueForm}) to get
$$f(z_\al)-f(x)=(\grad f(x+\gm (z_\al-x)))^T(z_\al-x)$$
for some $\gm \in [0,1]$. Since $z_\al-x=(1-\al)(y-x)$, this gives 
$$f(y)-f(x)\geq (\grad f(x+\gm (z_\al-x)))^T(y-x) \ .$$
Letting $\al\ra 1$ we have $z_\al \ra x$, that will give (\ref{Thm:ConvexFunctionMainTheorem:Eq:FirstOrder}).

(2) We extend the above argument using strong convexity (\ref{Eq:StronglyConvexFunction}). This means we have
$$f(z_\al)\leq \al f(x)+(1-\al)f(y)-\dfrac{1}{2}m\al (1-\al)\|x-y\|_2^2 \ ,$$
which gives
$$f(z_\al)-f(x)+\dfrac{m}{2}\al(1-\al)\|x-y\|^2 \leq (1-\al)(f(y)-f(x)) \ .$$
By (\ref{Thm:Taylor:Eq:MeanValueForm}) again we have
$$(1-\al)(\grad f(x+\gm(z_\al-x)))^T (y-x)+\dfrac{m}{2}\al(1-\al)\|y-x\|^2\leq (1-\al)(f(y)-f(x)) \ .$$
Dividing both sides by $1-\al$ and setting $\al \ra 1$ we get (\ref{Thm:ConvexFunctionMainTheorem:Eq:SecondOrderLowerBound}).

(3) We app;y (\ref{Thm:Taylor:Eq:IntegralForm}) to get
$$f(y)=f(x)+\int_0^1 (\grad f(x+\gm(y-x)))^T (y-x)d\gm \ .$$
This gives us
$$\begin{array}{ll}
& f(y)-f(x)-(\grad f(x))^T(y-x)
\\
= & \play{\int_0^1 [(\grad f(x+\gm(y-x)))^T-(\grad f(x))^T](y-x)d\gm}
\\
\leq & \play{\int_0^1 \|(\grad f(x+\gm(y-x)))^T-(\grad f(x))^T\|\cdot \|y-x\| d\gm}
\\
\leq & \play{\int_0^1 L\gm \|y-x\|^2 d\gm=\dfrac{L}{2}\|y-x\|^2 \ ,}
\end{array}$$
which is (\ref{Thm:ConvexFunctionMainTheorem:Eq:SecondOrderUpperBound}).
\end{proof}

We use the symbol $A \succeq B$ ($A \preceq B$) to mean $\lb_A\geq \lb_B$ ($\lb_A \leq \lb_B$) for every corresponding pair of eigenvalues of $A, B$.

\begin{theorem}\label{Thm:ConvexFunctionHessianCharacteristics}
Suppose that the function $f$ is twice continuously differentiable on $\R^n$. Then

(1) $f$ is strongly convex with modulus of convexity $m$ if and only if $\grad^2 f(x) \succeq mI$ for all $x$;

(2) $\grad f$ is Lipschitz continuous with Lipschitz constant $L$ if and only if $\grad^2 f(x) \preceq LI$ for all $x$.
\end{theorem}

\begin{proof}
(1) Suppose that the function $f$ is strongly--$m$ convex. 
Set $u\in \R^n$ and $\al>0$, then we consider the Taylor's expansion
\begin{equation}\label{Eq:Assignment2:TaylorExpansion}
f(x+\al u)=f(x)+\al \grad f(x)^T u+\dfrac{1}{2}\al^2 u^T\grad^2 f(x+t\al u)u 
\end{equation}
for some $0\leq t\leq 1$. We apply \eqref{Thm:ConvexFunctionMainTheorem:Eq:SecondOrderLowerBound} with $y=x+\al u$ so that
\begin{equation}\label{Eq:Assignment2:TaylorExpansionStrongConvexity}
f(x+\al u)\geq f(x)+\al (\grad f(x))^T u+\dfrac{m}{2}\al^2 \|u\|^2 \ .
\end{equation}
Comparing \eqref{Eq:Assignment2:TaylorExpansion} and \eqref{Eq:Assignment2:TaylorExpansionStrongConvexity} we see that for arbitrary choice of $u\in \R^n$ we have
$$u^T \grad^2 f(x+t\al u)u\geq m \|u\|^2 \ .$$
This implies that $\grad^2 f(x) \succeq mI$ as claimed. This shows the ``only if" part.

Indeed one can also show the ``if" part. To this end assume that $\grad^2 f(x) \succeq mI$. Then for any $z\in \R^n$ we have that $(x-z)^T\grad^2 f(z+t(x-z))(x-z)\geq m\|x-z\|^2$. Thus
\begin{equation}\label{Eq:Assignment2:mStrongConvexityDerivativex-z}
\begin{array}{ll}
f(x) & =f(z)+(\grad f(z))^T(x-z)+\dfrac{1}{2}(x-z)^T\grad^2f(z+t(x-z))(x-z)
\\
&\geq f(z)+(\grad f(z))^T(x-z)+\dfrac{m}{2}\|x-z\|^2 \ .
\end{array} 
\end{equation}
Similarly
\begin{equation}\label{Eq:Assignment2:mStrongConvexityDerivativey-z}
\begin{array}{ll}
f(y) & =f(z)+(\grad f(z))^T(y-z)+\dfrac{1}{2}(y-z)^T\grad^2f(z+t(y-z))(y-z)
\\
&\geq f(z)+(\grad f(z))^T(y-z)+\dfrac{m}{2}\|y-z\|^2 \ .
\end{array}
\end{equation}
We consider $(1-\al)$\eqref{Eq:Assignment2:mStrongConvexityDerivativex-z}$+\al$\eqref{Eq:Assignment2:mStrongConvexityDerivativey-z} and we set $z=(1-\al) x+\al y$. This gives
\begin{equation}\label{Eq:Assignment2:mStrongConvexityDerivative1-alphax-zalphay-z}
\begin{array}{l}
(1-\al) f(x)+\al f(y)
\\
\geq (\al+(1-\al))f(z)+
(\grad f(z))^T((1-\al) (x-z)+\al(y-z))+\dfrac{m}{2}\left((1-\al)\|x-z\|^2+\al\|y-z\|^2\right)
\\
= f(z)+
(\grad f(z))^T((1-\al) (x-z)+\al(y-z))+\dfrac{m}{2}\left((1-\al)\|x-z\|^2+\al\|y-z\|^2\right) \ .
\end{array} 
\end{equation}
Since $x-z=\al (x-y)$ and $y-z=(1-\al)(y-x)$, we see that $((1-\al)(x-z)+\al(y-z))=0$. Moreover, this means that
$$(1-\al)\|x-z\|^2+\al \|y-z\|^2=\left[(1-\al)\al^2+\al(1-\al)^2\right]\|x-y\|^2=\al(1-\al)\|x-y\|^2 \ .$$
From these we see that \eqref{Eq:Assignment2:mStrongConvexityDerivative1-alphax-zalphay-z} is the same as saying
$$(1-\al) f(x)+\al f(y)\geq f((1-\al)x+\al y)+\dfrac{m}{2}\al(1-\al)\|x-y\|^2 \ ,$$
which is \eqref{Eq:StronglyConvexFunction}.

(2) We want to show $\|\nabla f(y) - \nabla f(x)\| \leq L\|y-x\|$ is equivalent to $\nabla^2 f(x) \preceq LI$. Assume $\nabla^2 f(x) \preceq LI$, which means the spectral norm $\|\nabla^2 f(x)\|_2 \leq L$. Using (\ref{Thm:Taylor:Eq:GradientIntegralForm}) we get
$$ \begin{array}{ll}
\|\nabla f(y) - \nabla f(x)\| & = \play{\left\|\int_0^1 \nabla^2 f(x + t(y-x)) (y-x) dt \right\|}
\\
& \leq \play{\int_0^1 \left\|\nabla^2 f(x + t(y-x))\right\|\cdot \left\|y-x\right\| dt }
\\
& \leq \play{\int_0^1 L\|y-x\|dt}=L\|y-x\| \ ,
\end{array}$$
which proves $\nabla f$ is $L$-Lipschitz continuous. 
Assume $\nabla f$ is $L$-Lipschitz. For any vector $v$, consider the limit:
$$ \nabla^2 f(x) v = \lim_{t \to 0} \frac{\nabla f(x + tv) - \nabla f(x)}{t} $$
Taking the norm:
$$ \|\nabla^2 f(x) v\| = \lim_{t \to 0} \frac{\|\nabla f(x + tv) - \nabla f(x)\|}{t} \leq \lim_{t \to 0} \frac{L \|tv\|}{t} = L\|v\| $$
This implies the operator norm $\|\nabla^2 f(x)\|_2 \leq L$, which for a symmetric Hessian matrix is equivalent to $-LI \preceq \nabla^2 f(x) \preceq LI$. Since we usually deal with convex/semi-convex functions in this context, it implies $\nabla^2 f(x) \preceq LI$.
\end{proof}

\begin{theorem}[Strongly convex functions have unique minimizers]\label{Thm:MinimizerStronglyConvexFunctions}
Let $f$ be differentiable and strongly convex with modulus $m>0$. Then the minimizer $x^*$ of $f$ exists and is unique.
\end{theorem}

\begin{proof}
Uniqueness. Suppose there exist two distinct minimizers $x^*$ and $z^*$. Since $f$ is differentiable, we must have $\nabla f(x^*) = 0$ and $\nabla f(z^*) = 0$. Using the strong convexity property (\ref{Thm:ConvexFunctionMainTheorem:Eq:SecondOrderLowerBound}) at $x^*$:
$$ f(z^*) \geq f(x^*) + \nabla f(x^*)^T(z^* - x^*) + \frac{m}{2}\|z^* - x^*\|^2 = f(x^*) + \frac{m}{2}\|z^* - x^*\|^2 \ . $$
Similarly, using strong convexity at $z^*$:
$$ f(x^*) \geq f(z^*) + \frac{m}{2}\|x^* - z^*\|^2 \ . $$
Adding these two inequalities yields $f(x^*) + f(z^*) \geq f(x^*) + f(z^*) + m\|x^* - z^*\|^2$, which simplifies to $0 \geq m\|x^* - z^*\|^2$. Since $m > 0$, it must be that $\|x^* - z^*\| = 0$, hence $x^* = z^*$.

Existence. From the strong convexity definition, $f(y) \to \infty$ as $\|y\| \to \infty$ (coercivity). Since $f$ is continuous and coercive, it must attain a minimum on any closed set in $\mathbb{R}^n$.
\end{proof}

Below are a few more technical issues regarding convex and strongly convex functions.

\begin{theorem}\label{Thm:GeneralizedStrongConvexity}
Let $f$ be convex and continuously differentiable and $\grad f$ be $L$--Lipschitz. Then for any
$x,y\in \text{dom}(f)$ the following bounds hold:
\begin{equation}\label{Thm:GeneralizedStrongConvexity:Eq:UpperBound}
f(x)+\grad f(x)^T(y-x)+\dfrac{1}{2L}\|\grad f(x)-\grad f(y)\|^2\leq f(y) \ ,
\end{equation}

\begin{equation}\label{Thm:GeneralizedStrongConvexity:Eq:TwoSidedBound}
\dfrac{1}{L}\|\grad f(x)-\grad f(y)\|^2\leq (\grad f(x)-\grad f(y))^T(x-y)\leq L\|x-y\|^2 \ .
\end{equation}

In addition, let $f$ be strongly convex with modulus $m$ and unique minimizer $x^*$. Then for any $x,y\in \text{dom}(f)$
we have that

\begin{equation}\label{Thm:GeneralizedStrongConvexity:Eq:GeneralizedStrongConvexity}
f(y)-f(x)\geq -\dfrac{1}{2m}\|\grad f(x)\|^2 \ .
\end{equation}
\end{theorem}

\begin{proof} 
Define an auxiliary function $g(z) = f(z) - \nabla f(x)^T z$. Since $f$ is convex, $g$ is also convex. Furthermore, $\nabla g(z) = \nabla f(z) - \nabla f(x)$, which is also $L$-Lipschitz continuous. Note that $\nabla g(x) = 0$, implying $x$ is a global minimizer of $g(z)$ via Theorem \ref{Thm:MinimizerConvexFunctions}.
Using (\ref{Thm:ConvexFunctionMainTheorem:Eq:SecondOrderUpperBound}) we get
$$ g(z) \leq g(y) + \nabla g(y)^T (z - y) + \frac{L}{2} \|z - y\|^2 \ .$$
Setting $z = y - \dfrac{1}{L}\nabla g(y)$, and since $x$ minimizes $g$, we obtain:
$$ g(x) \leq g(y) - \frac{1}{2L} \|\nabla g(y)\|^2 \ .$$
Substituting $g(z) = f(z) - \nabla f(x)^T z$ back into the inequality:
$$ f(x) - \nabla f(x)^T x \leq f(y) - \nabla f(x)^T y - \frac{1}{2L} \|\nabla f(y) - \nabla f(x)\|^2  \ ,$$
which is (\ref{Thm:GeneralizedStrongConvexity:Eq:UpperBound}).

We swap $x$ and $y$ in (\ref{Thm:GeneralizedStrongConvexity:Eq:UpperBound}) to get
$$ f(y) + \nabla f(y)^T(x - y) + \frac{1}{2L}\|\nabla f(y) - \nabla f(x)\|^2 \leq f(x) \ .$$
Adding this to (\ref{Thm:GeneralizedStrongConvexity:Eq:UpperBound}) and canceling $f(x) + f(y)$ from both sides we get
$$ \nabla f(x)^T(y - x) + \nabla f(y)^T(x - y) + \frac{1}{L}\|\nabla f(x) - \nabla f(y)\|^2 \leq 0  \ ,$$
which gives the first inequality in (\ref{Thm:GeneralizedStrongConvexity:Eq:TwoSidedBound}) by rearranging.

The second inequality in (\ref{Thm:GeneralizedStrongConvexity:Eq:TwoSidedBound}) follows directly from Cauchy-Schwarz and the $L$-Lipschitz property:
$$ (\nabla f(x) - \nabla f(y))^T(x - y) \leq \|\nabla f(x) - \nabla f(y)\| \cdot \|x - y\| \leq L\|x - y\|^2  \ .$$

Assume $f$ is strongly convex with modulus $m$. Then we have
\begin{align*}
f(y) &\geq f(x) + \nabla f(x)^T (y-x) + \frac{m}{2} \|y-x\|^2 \\
&= f(x) + \frac{m}{2} \left[ \|y-x\|^2 + \frac{2}{m} \nabla f(x)^T (y-x) \right] \\
&= f(x) + \frac{m}{2} \left[ \|y-x\|^2 + 2\left( \frac{1}{m} \nabla f(x) \right)^T (y-x) + \frac{1}{m^2} \|\nabla f(x)\|^2 \right] - \frac{1}{2m} \|\nabla f(x)\|^2 \\
&= f(x) + \frac{m}{2} \left\| y-x + \frac{1}{m} \nabla f(x) \right\|^2 - \frac{1}{2m} \|\nabla f(x)\|^2 \ ,
\end{align*}
which gives (\ref{Thm:GeneralizedStrongConvexity:Eq:GeneralizedStrongConvexity}).
\end{proof}

\begin{definition}[Generalized Strong Convexity]\label{Def:GeneralizedStrongConvexity}
We say the convex function $f(x)$ has \emph{Generalized Strong Convexity} if
\begin{equation}\label{Eq:GeneralizedStrongConvexity}
\|\grad f(x)\|^2\geq 2m[f(x)-f^*] \text{ for some } m>0 \ ,
\end{equation}
where $f^*=f(x^*)$ is the minimum of $f$. 
\end{definition}

The Generalized Strong Convexity holds in situations other than when $f$ is strongly convex.

\section{Optimality Conditions}
\label{Ch:ConvexFunctions:OptimalityCondition}

We derive optimality conditions for the smooth unconstrained problem

\begin{equation}\label{Eq:SmoothUnconstrainedOptimization}
\min\li_{x\in \R^n}f(x) \ ,
\end{equation}
where $f(x)$ is a smooth function.

\begin{theorem}[Necessary conditions for smooth unconstrained optimization]\label{Thm:NecessaryConditionsSmoothUnconstrainedOptimization}
We have 
\begin{itemize}
\item[(a)] (first--order necessary condition) Suppose that $f$ is continuously differentiable. Then if $x^*$ is a local minimizer of \eqref{Eq:SmoothUnconstrainedOptimization}, then $\grad f(x^*)=0$;

\item[(b)] (second--order necessary condition) Suppose that $f$ is twice continuously differentiable. Then if $x^*$ is a local minimizer of \eqref{Eq:SmoothUnconstrainedOptimization}, then $\grad f(x^*)=0$ and $\grad^2 f(x^*)$ is positive definite.
\end{itemize}
\end{theorem}

\begin{proof}
Suppose $x^*$ is a local minimizer of $f$. Then for any direction $d \in \mathbb{R}^n$ and sufficiently small step size $\alpha > 0$, we have $f(x^* + \alpha d) \geq f(x^*)$.

(a) By (\ref{Thm:Taylor:Eq:GradientMeanValueForm}) we have
$$ f(x^* + \alpha d) = f(x^*) + \alpha \nabla f(x^*)^T d + O(\alpha^2) $$
Since $f(x^* + \alpha d) \geq f(x^*)$, it follows that $\alpha \nabla f(x^*)^T d + O(\alpha^2) \geq 0$. Dividing by $\alpha$ and letting $\alpha \to 0$, we obtain $\nabla f(x^*)^T d \geq 0$. Since this holds for any $d$, we choose $d = -\nabla f(x^*)$, which gives $-\|\nabla f(x^*)\|^2 \geq 0$. Thus, $\nabla f(x^*) = 0$.

(b) Given $\nabla f(x^*) = 0$, by (\ref{Thm:Taylor:Eq:GradientMeanValueForm}) we get
$$ f(x^* + \alpha d) = f(x^*) + \frac{\alpha^2}{2} d^T \nabla^2 f(x^*) d + o(\alpha^2) $$
For $f(x^* + \alpha d) \geq f(x^*)$ to hold for small $\alpha$, we must have $d^T \nabla^2 f(x^*) d \geq 0$ for all $d$. This implies that the Hessian matrix $\nabla^2 f(x^*)$ is positive semi-definite.
\end{proof}

\begin{theorem}[Sufficient conditions for convex functions in smooth unconstrained optimization]
\label{Thm:SufficientConditionsSmoothUnconstrainedOptimizationConvex}
If $f$ is continuously differentiable and convex, then if $\grad f(x^*)=0$, then $x^*$ is a global minimizer of \eqref{Eq:SmoothUnconstrainedOptimization}.
In addition, if $f$ is strongly convex, then $x^*$ is the unique global minimizer.
\end{theorem}

\begin{proof}
Assume $f$ is convex and $\nabla f(x^*) = 0$. By (\ref{Thm:ConvexFunctionMainTheorem:Eq:FirstOrder}), for any $x \in \mathbb{R}^n$ we have
$$ f(x) \geq f(x^*) + \nabla f(x^*)^T(x - x^*) \ . $$
Since $\nabla f(x^*) = 0$, this gives $ f(x) \geq f(x^*)$. This proves that $x^*$ is a global minimizer. If $f$ is further assumed to be strongly convex, then $x^*$ is the unique global minimizer due to Theorem \ref{Thm:MinimizerStronglyConvexFunctions}.
\end{proof}

\begin{theorem}[Sufficient conditions for nonconvex functions in smooth unconstrained optimization]
Suppose that $f$ is twice continuously differentiable and that for some $x^*$ we have $\grad f(x^*)=0$ and $\grad^2 f(x^*)$ is positive 
definite, then $x^*$ is a strict local minimizer of \eqref{Eq:SmoothUnconstrainedOptimization}.
\end{theorem}

\begin{proof}
Suppose $\nabla f(x^*) = 0$ and $\nabla^2 f(x^*)$ is positive definite. Let $\lambda_{\min} > 0$ be the smallest eigenvalue of $\nabla^2 f(x^*)$. Then for any $d \neq 0$:
$$ d^T \nabla^2 f(x^*) d \geq \lambda_{\min} \|d\|^2 \ .$$
Using (\ref{Thm:Taylor:Eq:GradientMeanValueForm}), the second-order Taylor expansion around $x^*$, we get
$$ f(x^* + d) = f(x^*) + \nabla f(x^*)^T d + \frac{1}{2} d^T \nabla^2 f(x^*) d + o(\|d\|^2) \ .$$
Since $\nabla f(x^*) = 0$, we have:
$$ f(x^* + d) \geq f(x^*) + \frac{\lambda_{\min}}{2} \|d\|^2 + o(\|d\|^2) $$
For sufficiently small $\|d\|$, the term $\dfrac{\lambda_{\min}}{2} \|d\|^2$ dominates the higher-order terms $o(\|d\|^2)$, ensuring $f(x^* + d) > f(x^*)$. Thus, $x^*$ is a strict local minimizer.
\end{proof}

\section{Experiment: Nonconvexity of the Loss Landscape of Neural Networks}
\label{Ch:ConvexFunctions:Experiment:NonConvexLossNN}

Consider a neural network with one hidden layer that consists of $p$ neurons and input $x\in \mathbb{R}^1$, output $y\in \mathbb{R}^1$. The neural network function has the form 
$y(x)=\sum\limits_{j=1}^p c_j \sigma(a_j x- b_j) \ , $
where $\mathbf{a}=a_j, \mathbf{b}=b_j$ and $\mathbf{c}=c_j$ are the neural network weights. Assume $(a_j, b_j, c_j)\sim \mathcal{N}(0, I_3)$, $j=1,2,â€¦,p$ is a family of i.i.d multivariate normal distributions. For different realizations of $(a_j, b_j, c_j)$, we plot the function $y(x)$ on $x$-$y$ graph. We have experimented different hidden layer sizes.

With the above done, assume that the training data $(x, y)$ follows a bivariate normal distribution $\mathcal{N}(0, I_2)$. Let the variables $a_1$ and $a_2$ vary in a certain interval. Let all other neural network weights $a_j, b_j, c_j$ follow the same assumption as above. For different $(a_1, a_2)$, we plot the empirical loss function of this network as a function of $(a_1, a_2)$. We can see that the loss function may not be convex with respect to $(a_1, a_2)$. We show experiment results for different hidden layer sizes.

We refer to \cite{ComputationalOptimizationGithubRepo},  \verb#src/one_hidden_layer_nn#.

A typical non-convex loss landscape looks as follows:

\begin{figure}[H]
\centering
\includegraphics[height=5.8cm, width=9cm]{figures/Fig_OneHiddenLayerNN-Loss_Sigmoid_layersize=20, training size=10}
\caption{Loss Landscape of a neural network.}
\label{Fig:NN-Loss}
\end{figure}



